"""Caching functions."""
import dataclasses
import hashlib
import json
import os
import shutil
import time
from pathlib import Path

from app import service
from app.constants import CACHE_CHECK_INTERVAL, CACHE_CHECK_TIMEOUT, SAMPLING_RATIO
from app.logger import L


@dataclasses.dataclass(frozen=True)
class CacheParams:
    """Parameters to be considered for cache."""

    input_path: Path
    population_name: str | None
    attributes: list[str]
    sampling_ratio: float
    seed: int

    def json(self) -> str:
        """Return a json representation of the object."""
        return json.dumps(
            {
                "input_path": str(self.input_path),
                "population_name": self.population_name,
                "attributes": self.attributes,
                "sampling_ratio": self.sampling_ratio,
                "seed": self.seed,
            }
        )

    def checksum(self) -> str:
        """Calculate the checksum of the object."""
        return hashlib.sha256(self.json().encode("utf-8")).hexdigest()


@dataclasses.dataclass(frozen=True)
class CachePaths:
    """Collection of paths used for caching."""

    base: Path

    @property
    def nodes(self):
        """Return the nodes file path."""
        return self.base / "nodes.h5"

    @property
    def metadata(self):
        """Return the metadata file path."""
        return self.base / "metadata.json"

    @property
    def ok(self):
        """Return the OK file path."""
        return self.base / "OK"


def _read_cache(paths: CachePaths) -> None:
    """Wait until the OK file is ready, in case it's being generated by a concurrent process."""
    L.info(f"Reading cache {paths.base.name}")
    counter = CACHE_CHECK_TIMEOUT // CACHE_CHECK_INTERVAL
    while not paths.ok.exists() and counter > 0:
        time.sleep(CACHE_CHECK_INTERVAL)
        counter -= 1
    if not paths.ok.exists():
        raise RuntimeError("Timeout while waiting to read the cache")


def _write_cache(paths: CachePaths, params: CacheParams) -> None:
    """Write the cache and the OK file."""
    L.info(f"Writing cache {paths.base.name}")
    try:
        paths.metadata.write_text(params.json(), encoding="utf-8")
        service.downsample(
            input_path=params.input_path,
            output_path=paths.nodes,
            population_name=params.population_name,
            sampling_ratio=params.sampling_ratio,
            seed=params.seed,
            attributes=params.attributes,
        )
        paths.ok.touch(exist_ok=False)
    except BaseException:
        L.error("Failure to write the cache, removing any temporary file")
        shutil.rmtree(paths.base)
        raise


def check_cache(params: CacheParams) -> CacheParams:
    """Read or write the cache."""
    if params.sampling_ratio > SAMPLING_RATIO:
        L.info(
            f"Not using the cache because the sampling_ratio {params.sampling_ratio} "
            f"is greater than {SAMPLING_RATIO}"
        )
        return params

    # cache SAMPLING_RATIO, to reuse the same cache even when passing a lower sampling_ratio
    params_to_cache = dataclasses.replace(params, sampling_ratio=SAMPLING_RATIO)
    checksum = params_to_cache.checksum()
    paths = CachePaths(base=Path(os.getenv("TMPDIR", "/tmp"), "cached_circuits", checksum))
    try:
        paths.base.mkdir(parents=True, exist_ok=False)
    except OSError:
        if not paths.base.is_dir():
            raise
        is_cached = True
    else:
        is_cached = False
    if is_cached:
        _read_cache(paths)
    else:
        _write_cache(paths, params_to_cache)
    return dataclasses.replace(
        params,
        input_path=paths.nodes,
        sampling_ratio=params.sampling_ratio / params_to_cache.sampling_ratio,
    )
